{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## PAGASA BRBFFWC ARCHIVE CORRECTION\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General advisory: Change the file directory for each of the code snippet**\n",
    "\n",
    "find for `base_path` in each code snippet\n",
    "\n",
    "Where RAW files must be inside the directory\n",
    "\n",
    "The sequence of codes must be executed **IN ORDER** for it to work (since the correction code is the basis for the other functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**` CORRECTION CODE FOR MISSING DATA AND OTHERS `**\n",
    "\n",
    "This code snippet does the following:\n",
    "\n",
    "* Fills up missing minute-datasets between values, and then puts \"not counted (NC) as a remark\n",
    "* Gives us the accumulation (xx:10 to x(x+1):00) for the following day and determines if accumulation occurs\n",
    "* Exports the data in both excel and text form for cross-verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Alanao.TXT ---\n",
      "Auto-detected Delimiter: '\\t', Date/Time Format: '%d/%m/%Y %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Alanao.txt\n",
      "\n",
      "\n",
      "--- Processing Alanao.TXT ---\n",
      "Auto-detected Delimiter: '\\t', Date/Time Format: '%d/%m/%Y %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Alanao.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Alanao.txt': 630381\n",
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Alanao.txt': 630381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Alanao_Exp.xlsx'.\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Alanao_Exp.xlsx'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Alanao_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Bagamelon.TXT ---\n",
      "Auto-detected Delimiter: '\\t', Date/Time Format: '%d/%m/%Y %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Bagamelon.txt\n",
      "Warning: Skipping malformed line 1 in D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Bagamelon.txt: '630374\t\t\tAccumulation\t10-min' - Error parsing date/time or value: time data '630374 ' does not match format '%d/%m/%Y %H:%M'\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Alanao_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Bagamelon.TXT ---\n",
      "Auto-detected Delimiter: '\\t', Date/Time Format: '%d/%m/%Y %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Bagamelon.txt\n",
      "Warning: Skipping malformed line 1 in D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Bagamelon.txt: '630374\t\t\tAccumulation\t10-min' - Error parsing date/time or value: time data '630374 ' does not match format '%d/%m/%Y %H:%M'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Bagamelon_Exp.xlsx'.\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Bagamelon_Exp.xlsx'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Bagamelon_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Balongay.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Balongay.txt\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Bagamelon_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Balongay.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Balongay.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Balongay.txt': 630375\n",
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Balongay.txt': 630375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Balongay_Exp.xlsx'.\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Balongay_Exp.xlsx'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Balongay_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Bato.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Bato.txt\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Balongay_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Bato.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Bato.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Bato.txt': 630364\n",
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Bato.txt': 630364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Bato_Exp.xlsx'.\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Bato_Exp.xlsx'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Bato_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Buhi.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Buhi.txt\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Bato_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Buhi.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Buhi.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Buhi.txt': 630365\n",
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Buhi.txt': 630365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Buhi_Exp.xlsx'.\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Buhi_Exp.xlsx'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Buhi_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Caguscos.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Caguscos.txt\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Buhi_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Caguscos.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Caguscos.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Caguscos.txt': 630363\n",
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Caguscos.txt': 630363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Caguscos_Exp.xlsx'.\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Caguscos_Exp.xlsx'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Caguscos_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Calzada.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Calzada.txt\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Caguscos_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Calzada.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Calzada.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Calzada.txt': 630362\n",
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Calzada.txt': 630362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Calzada_Exp.xlsx'.\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Calzada_Exp.xlsx'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Calzada_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Camaligan.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Camaligan.txt\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Calzada_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Camaligan.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Camaligan.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Camaligan.txt': 630373\n",
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Camaligan.txt': 630373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Camaligan_Exp.xlsx'.\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Camaligan_Exp.xlsx'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Camaligan_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Malabog.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Malabog.txt\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Camaligan_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Malabog.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Malabog.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Malabog.txt': 630361\n",
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Malabog.txt': 630361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Malabog_Exp.xlsx'.\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Malabog_Exp.xlsx'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Malabog_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Ocampo.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Ocampo.txt\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Malabog_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Ocampo.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Ocampo.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Ocampo.txt': 630372\n",
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Ocampo.txt': 630372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Ocampo_Exp.xlsx'.\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Ocampo_Exp.xlsx'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Ocampo_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Ombao.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Ombao.txt\n",
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Ombao.txt': 630371\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Ocampo_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Ombao.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Ombao.txt\n",
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Ombao.txt': 630371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Ombao_Exp.xlsx'.\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Ombao_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Sipocot.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Sipocot.txt\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Ombao_Exp.xlsx'.\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Ombao_Exp_Corrected.txt'.\n",
      "\n",
      "--- Processing Sipocot.TXT ---\n",
      "Auto-detected Delimiter: ',', Date/Time Format: '%Y/%m/%d %H:%M' for D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Sipocot.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Sipocot.txt': 630382\n",
      "Note: Initial header line from input file 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Sipocot.txt': 630382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Sipocot_Exp.xlsx'.\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Sipocot_Exp.xlsx'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Sipocot_Exp_Corrected.txt'.\n",
      "Data processing complete. Output written to 'D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\Sipocot_Exp_Corrected.txt'.\n"
     ]
    }
   ],
   "source": [
    "# ----------------- HERE ---------------- #\n",
    "import datetime\n",
    "import collections\n",
    "import os\n",
    "import openpyxl\n",
    "\n",
    "def _detect_delimiter_and_format(lines_to_sample, potential_delimiters=None, potential_date_formats=None):\n",
    "    \"\"\"\n",
    "    Analyzes sample lines to detect the most likely delimiter and date format.\n",
    "    \"\"\"\n",
    "    if potential_delimiters is None:\n",
    "        potential_delimiters = ['\\t', ',']\n",
    "\n",
    "    if potential_date_formats is None:\n",
    "        potential_date_formats = [\"%Y/%m/%d\", \"%d/%m/%Y\"]\n",
    "\n",
    "    best_delimiter = None\n",
    "    best_date_format_str = None\n",
    "    max_successful_parses = -1\n",
    "\n",
    "    for delim in potential_delimiters:\n",
    "        for date_fmt in potential_date_formats:\n",
    "            successful_parses = 0\n",
    "            for line in lines_to_sample:\n",
    "                line_parts = line.strip().split(delim)\n",
    "                if len(line_parts) >= 4:\n",
    "                    date_str = line_parts[0]\n",
    "                    time_str = line_parts[1]\n",
    "                    try:\n",
    "                        datetime.datetime.strptime(f\"{date_str} {time_str}\", f\"{date_fmt} %H:%M\")\n",
    "                        float(line_parts[3])\n",
    "                        successful_parses += 1\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            \n",
    "            if successful_parses > max_successful_parses:\n",
    "                max_successful_parses = successful_parses\n",
    "                best_delimiter = delim\n",
    "                best_date_format_str = f\"{date_fmt} %H:%M\"\n",
    "\n",
    "    if max_successful_parses == 0:\n",
    "        raise ValueError(\"Could not detect suitable delimiter and date format from sample lines. Please ensure the file has data in expected formats.\")\n",
    "\n",
    "    return best_delimiter, best_date_format_str\n",
    "\n",
    "def get_daily_accumulation_end_dt(record_dt):\n",
    "    \"\"\"\n",
    "    Determines the 00:00 datetime that marks the end of the daily accumulation period\n",
    "    to which the given record_dt contributes.\n",
    "    \"\"\"\n",
    "    if record_dt.hour == 0 and record_dt.minute == 0:\n",
    "        return record_dt\n",
    "    next_day = record_dt.date() + datetime.timedelta(days=1)\n",
    "    return datetime.datetime(next_day.year, next_day.month, next_day.day, 0, 0)\n",
    "\n",
    "\n",
    "def process_data_file(input_filepath, output_excel_filepath, output_txt_filepath, \n",
    "                      start_point=None, end_point=None):\n",
    "    \"\"\"\n",
    "    Reads a text file with timestamped data, auto-detects delimiter and date format,\n",
    "    fills completeness in 10-minute intervals, calculates 10-min differences,\n",
    "    hourly accumulations, and accumulation determinants.\n",
    "    Finally, calculates and includes daily accumulation based on 00:10 to 00:00 (next day) window,\n",
    "    saving the output to both an Excel (.xlsx) and a text (.txt) file.\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): Path to the input text file.\n",
    "        output_excel_filepath (str): Path to the output Excel file (.xlsx).\n",
    "        output_txt_filepath (str): Path to the output text file (.txt).\n",
    "        start_point (datetime.datetime, optional): The explicit start date and time for processing.\n",
    "                                                     If None, the earliest datetime from the input file\n",
    "                                                     (rounded to nearest 10-min interval) is used.\n",
    "        end_point (datetime.datetime, optional): The explicit end date and time for processing.\n",
    "                                                   If None, the latest datetime from the input file\n",
    "                                                   (rounded to nearest 10-min interval) is used.\n",
    "    \"\"\"\n",
    "\n",
    "    # Stage 1: Parse input file and store actual data\n",
    "    actual_data_records = collections.OrderedDict()\n",
    "    \n",
    "    initial_header_line = None\n",
    "    start_data_line_index = 0\n",
    "    file_delimiter = None\n",
    "    input_datetime_format = None\n",
    "\n",
    "    try:\n",
    "        with open(input_filepath, 'r') as infile:\n",
    "            lines = infile.readlines()\n",
    "            \n",
    "            if lines:\n",
    "                first_line_stripped = lines[0].strip()\n",
    "                temp_parts_tab = first_line_stripped.split('\\t')\n",
    "                temp_parts_comma = first_line_stripped.split(',')\n",
    "\n",
    "                is_header_tab = len(temp_parts_tab) >= 1 and temp_parts_tab[0].isdigit() and all(p == '' for p in temp_parts_tab[1:])\n",
    "                is_header_comma = len(temp_parts_comma) >= 1 and temp_parts_comma[0].isdigit() and all(p == '' for p in temp_parts_comma[1:])\n",
    "                \n",
    "                if is_header_tab or is_header_comma:\n",
    "                    initial_header_line = lines[0]\n",
    "                    start_data_line_index = 1\n",
    "                \n",
    "                sample_lines = lines[start_data_line_index : min(start_data_line_index + 20, len(lines))]\n",
    "                \n",
    "                if not sample_lines:\n",
    "                    print(f\"Error: No data lines found in the input file '{input_filepath}' for format detection.\")\n",
    "                    return\n",
    "\n",
    "                try:\n",
    "                    file_delimiter, input_datetime_format = _detect_delimiter_and_format(sample_lines)\n",
    "                    print(f\"Auto-detected Delimiter: {repr(file_delimiter)}, Date/Time Format: '{input_datetime_format}' for {input_filepath}\")\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error during auto-detection for {input_filepath}: {e}\")\n",
    "                    print(\"Attempting to proceed with default settings (tab delimiter, DD/MM/YYYY HH:MM).\")\n",
    "                    file_delimiter = '\\t'\n",
    "                    input_datetime_format = \"%d/%m/%Y %H:%M\"\n",
    "\n",
    "            for line_num, line in enumerate(lines[start_data_line_index:]):\n",
    "                line_parts = line.strip().split(file_delimiter)\n",
    "                \n",
    "                if len(line_parts) >= 4:\n",
    "                    try:\n",
    "                        current_datetime = datetime.datetime.strptime(f\"{line_parts[0]} {line_parts[1]}\", input_datetime_format)\n",
    "                        \n",
    "                        try:\n",
    "                            reading_value = float(line_parts[3])\n",
    "                        except ValueError:\n",
    "                            reading_value = None\n",
    "                        \n",
    "                        actual_data_records[current_datetime] = reading_value\n",
    "\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Warning: Skipping malformed line {line_num + start_data_line_index + 1} in {input_filepath}: '{line.strip()}' - Error parsing date/time or value: {e}\")\n",
    "                    except IndexError:\n",
    "                        print(f\"Warning: Skipping incomplete line {line_num + start_data_line_index + 1} in {input_filepath}: '{line.strip()}' - Not enough columns.\")\n",
    "                else:\n",
    "                    print(f\"Warning: Skipping malformed line {line_num + start_data_line_index + 1} in {input_filepath}: '{line.strip()}' - Does not have expected number of columns.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file '{input_filepath}' not found. Please ensure the file is in the correct directory.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading the file '{input_filepath}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Stage 2: Determine full time range and fill missing intervals with Readings\n",
    "    all_parsed_datetimes = sorted(actual_data_records.keys())\n",
    "\n",
    "    if not all_parsed_datetimes:\n",
    "        print(f\"No valid data found or processed from the input file '{input_filepath}' to reconstruct.\")\n",
    "        return\n",
    "\n",
    "    first_data_dt_from_file = all_parsed_datetimes[0]\n",
    "    last_data_dt_from_file = all_parsed_datetimes[-1]\n",
    "\n",
    "    output_start_dt = start_point if start_point is not None else \\\n",
    "                      first_data_dt_from_file.replace(minute=(first_data_dt_from_file.minute // 10) * 10, second=0, microsecond=0)\n",
    "\n",
    "    output_end_dt = end_point if end_point is not None else \\\n",
    "                    last_data_dt_from_file.replace(minute=(last_data_dt_from_file.minute // 10) * 10, second=0, microsecond=0)\n",
    "\n",
    "    if output_start_dt.minute % 10 != 0:\n",
    "        output_start_dt = output_start_dt + datetime.timedelta(minutes=(10 - (output_start_dt.minute % 10)))\n",
    "        output_start_dt = output_start_dt.replace(second=0, microsecond=0)\n",
    "    else:\n",
    "        output_start_dt = output_start_dt.replace(second=0, microsecond=0)\n",
    "\n",
    "    if output_end_dt.minute % 10 != 0:\n",
    "        output_end_dt = output_end_dt.replace(minute=(output_end_dt.minute // 10) * 10, second=0, microsecond=0)\n",
    "    else:\n",
    "        output_end_dt = output_end_dt.replace(second=0, microsecond=0)\n",
    "        \n",
    "    current_expected_dt = output_start_dt\n",
    "    \n",
    "    records_with_readings = []\n",
    "    \n",
    "    while current_expected_dt <= output_end_dt:\n",
    "        reading = actual_data_records.get(current_expected_dt)\n",
    "        \n",
    "        if reading is not None:\n",
    "            records_with_readings.append({\n",
    "                'datetime': current_expected_dt,\n",
    "                'reading': reading\n",
    "            })\n",
    "        else:\n",
    "            records_with_readings.append({\n",
    "                'datetime': current_expected_dt,\n",
    "                'reading': \"NC\"\n",
    "            })\n",
    "\n",
    "        current_expected_dt += datetime.timedelta(minutes=10)\n",
    "\n",
    "    records_with_readings.sort(key=lambda x: x['datetime'])\n",
    "\n",
    "    # Stage 3: Calculate 10-min differences\n",
    "    final_processed_records = []\n",
    "    previous_numeric_reading = None\n",
    "    \n",
    "    for i, record in enumerate(records_with_readings):\n",
    "        current_reading_val = record['reading']\n",
    "        ten_min_diff = \"NC\"\n",
    "\n",
    "        if current_reading_val != \"NC\":\n",
    "            if i == 0:\n",
    "                ten_min_diff = 0\n",
    "            elif previous_numeric_reading is not None:\n",
    "                ten_min_diff = current_reading_val - previous_numeric_reading\n",
    "            \n",
    "            previous_numeric_reading = current_reading_val \n",
    "        \n",
    "        final_processed_records.append({\n",
    "            'datetime': record['datetime'],\n",
    "            'reading': current_reading_val,\n",
    "            'ten_min_diff': ten_min_diff,\n",
    "            'hourly_accumulation': \"\",\n",
    "            'accumulation_determinant': \"\",\n",
    "            'daily_accumulation': \"\"\n",
    "        })\n",
    "\n",
    "    # Stage 4: Calculate Hourly Accumulation and Determinant (Modified)\n",
    "    current_hourly_window_diffs = collections.deque(maxlen=6) # Use deque to efficiently manage a sliding window\n",
    "\n",
    "    for record in final_processed_records:\n",
    "        ten_min_diff = record['ten_min_diff']\n",
    "        current_hourly_window_diffs.append(ten_min_diff)\n",
    "\n",
    "        # Only calculate hourly accumulation if the current time is at XX:00 and we have 6 data points\n",
    "        if record['datetime'].minute == 0 and len(current_hourly_window_diffs) == 6:\n",
    "            current_hourly_accumulation = 0\n",
    "            has_nc_in_window = False\n",
    "\n",
    "            for diff in current_hourly_window_diffs:\n",
    "                if diff == \"NC\":\n",
    "                    has_nc_in_window = True\n",
    "                    break\n",
    "                else:\n",
    "                    current_hourly_accumulation += diff\n",
    "\n",
    "            if has_nc_in_window:\n",
    "                record['hourly_accumulation'] = \"NC\"\n",
    "                record['accumulation_determinant'] = \"NC\"\n",
    "            else:\n",
    "                record['hourly_accumulation'] = current_hourly_accumulation\n",
    "                if current_hourly_accumulation == 0:\n",
    "                    record['accumulation_determinant'] = \"No Accumulation\"\n",
    "                elif current_hourly_accumulation > 0:\n",
    "                    record['accumulation_determinant'] = \"Accumulation\"\n",
    "                else:\n",
    "                    record['accumulation_determinant'] = \"Decline\"\n",
    "        else:\n",
    "            record['hourly_accumulation'] = \"\"\n",
    "            record['accumulation_determinant'] = \"\"\n",
    "\n",
    "\n",
    "    # Stage 5: Calculate Daily Accumulation (00:10 to 00:00 next day)\n",
    "    daily_sums_to_assign = collections.defaultdict(lambda: {'sum': 0.0, 'has_nc': False})\n",
    "\n",
    "    for record in final_processed_records:\n",
    "        hourly_acc = record['hourly_accumulation']\n",
    "        record_dt = record['datetime']\n",
    "\n",
    "        daily_period_end_dt = get_daily_accumulation_end_dt(record_dt)\n",
    "\n",
    "        # If it's a 00:00 record, it marks the end of a daily period for accumulation calculation.\n",
    "        # However, the summation itself should include values up to 23:50 of the current day\n",
    "        # and assign to 00:00 of the next day.\n",
    "        # The logic here is already accumulating correctly based on daily_period_end_dt.\n",
    "        # We need to ensure that when daily_period_end_dt is the next day's 00:00,\n",
    "        # the hourly_acc contributes to that specific daily sum.\n",
    "\n",
    "        if hourly_acc != \"\": # Only process actual hourly accumulation values\n",
    "            if hourly_acc == \"NC\":\n",
    "                daily_sums_to_assign[daily_period_end_dt]['has_nc'] = True\n",
    "            elif isinstance(hourly_acc, (int, float)):\n",
    "                if not daily_sums_to_assign[daily_period_end_dt]['has_nc']:\n",
    "                    daily_sums_to_assign[daily_period_end_dt]['sum'] += hourly_acc\n",
    "    \n",
    "    for record in final_processed_records:\n",
    "        record_dt = record['datetime']\n",
    "        # The daily accumulation should be assigned to the *next day's 00:00 record*\n",
    "        # or the current day's 00:00 if it is the start of the daily accumulation cycle.\n",
    "        # The key in daily_sums_to_assign is the *end* datetime of the daily accumulation.\n",
    "        # So, if record_dt is 00:00, we check for that as a key.\n",
    "\n",
    "        if record_dt.hour == 0 and record_dt.minute == 0:\n",
    "            if record_dt in daily_sums_to_assign:\n",
    "                daily_info = daily_sums_to_assign[record_dt]\n",
    "                if daily_info['has_nc']:\n",
    "                    record['daily_accumulation'] = \"NC\"\n",
    "                else:\n",
    "                    record['daily_accumulation'] = daily_info['sum']\n",
    "\n",
    "\n",
    "    # Stage 6: Write to output Excel and Text files\n",
    "\n",
    "    # Write to Excel (.xlsx)\n",
    "    try:\n",
    "        workbook = openpyxl.Workbook()\n",
    "        sheet = workbook.active\n",
    "        sheet.title = \"Processed Data\"\n",
    "\n",
    "        if initial_header_line:\n",
    "            print(f\"Note: Initial header line from input file '{input_filepath}': {initial_header_line.strip()}\")\n",
    "\n",
    "        headers = [\"Date\", \"Time\", \"Reading\", \"10-min\", \"Accumulation\", \"Determinant\", \"Daily Accumulation\"]\n",
    "        sheet.append(headers)\n",
    "\n",
    "        for record in final_processed_records:\n",
    "            row_data = [\n",
    "                record['datetime'].date(),\n",
    "                record['datetime'].time(),\n",
    "                record['reading'] if record['reading'] != \"NC\" else \"NC\",\n",
    "                record['ten_min_diff'] if record['ten_min_diff'] != \"NC\" else \"NC\",\n",
    "                record['hourly_accumulation'] if record['hourly_accumulation'] != \"NC\" else \"NC\",\n",
    "                record['accumulation_determinant'],\n",
    "                record['daily_accumulation'] if record['daily_accumulation'] != \"NC\" else \"NC\"\n",
    "            ]\n",
    "            sheet.append(row_data)\n",
    "            \n",
    "        workbook.save(output_excel_filepath)\n",
    "        print(f\"Data processing complete. Output written to '{output_excel_filepath}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing to the output Excel file '{output_excel_filepath}': {e}\")\n",
    "\n",
    "    # Write to Text (.txt)\n",
    "    try:\n",
    "        with open(output_txt_filepath, 'w') as outfile:\n",
    "            if initial_header_line:\n",
    "                outfile.write(initial_header_line)\n",
    "\n",
    "            outfile.write(f\"Date\\tTime\\tReading\\t10-min\\tAccumulation\\tDeterminant\\tDaily Accumulation\\n\")\n",
    "\n",
    "            for record in final_processed_records:\n",
    "                date_part_str = record['datetime'].strftime(\"%d/%m/%Y\")\n",
    "                \n",
    "                # Format time for text output to avoid leading zeros for hour 0 (0:00 vs 00:00)\n",
    "                if record['datetime'].hour == 0 and record['datetime'].minute == 0:\n",
    "                     time_part_str = \"0:00\"\n",
    "                else:\n",
    "                     time_part_str = record['datetime'].strftime(\"%H:%M\")\n",
    "\n",
    "                reading_output = str(record['reading'])\n",
    "                ten_min_diff_output = str(record['ten_min_diff'])\n",
    "                hourly_acc_output = str(record['hourly_accumulation'])\n",
    "                determinant_output = str(record['accumulation_determinant'])\n",
    "                daily_acc_output = str(record['daily_accumulation'])\n",
    "                \n",
    "                outfile.write(\n",
    "                    f\"{date_part_str}\\t{time_part_str}\\t{reading_output}\\t\"\n",
    "                    f\"{ten_min_diff_output}\\t{hourly_acc_output}\\t{determinant_output}\\t\"\n",
    "                    f\"{daily_acc_output}\\n\"\n",
    "                )\n",
    "        print(f\"Data processing complete. Output written to '{output_txt_filepath}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing to the output text file '{output_txt_filepath}': {e}\")\n",
    "\n",
    "\n",
    "# --- Processing all listed files ---\n",
    "# Create a dummy base_path for execution environment, as the original path D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\ does not exist\n",
    "# In a real scenario, base_path would point to the directory containing the input files.\n",
    "base_path = r\"D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\\" # Current directory\n",
    "\n",
    "# List of files and their explicit start points\n",
    "files_to_process = [\n",
    "    (\"Alanao\", datetime.datetime(2019, 4, 25, 11, 10)),\n",
    "    (\"Bagamelon\", datetime.datetime(2019, 4, 25, 14, 10)),\n",
    "    (\"Balongay\", datetime.datetime(2019, 4, 26, 11, 40)),\n",
    "    (\"Bato\", datetime.datetime(2020, 12, 17, 12, 10)),\n",
    "    (\"Buhi\", datetime.datetime(2020, 12, 17, 10, 10)),\n",
    "    (\"Caguscos\", datetime.datetime(2020, 12, 18, 9, 10)),\n",
    "    (\"Calzada\", datetime.datetime(2020, 12, 18, 13, 10)),\n",
    "    (\"Camaligan\", datetime.datetime(2019, 4, 26, 10, 30)),\n",
    "    (\"Malabog\", datetime.datetime(2020, 12, 18, 12, 10)),\n",
    "    (\"Ocampo\", datetime.datetime(2020, 12, 27, 10, 10)),\n",
    "    (\"Ombao\", datetime.datetime(2024, 12, 5, 15, 10)),\n",
    "    (\"Sipocot\", datetime.datetime(2019, 4, 25, 13, 10))]\n",
    "\n",
    "for file_name, start_dt in files_to_process:\n",
    "    print(f\"\\n--- Processing {file_name}.TXT ---\")\n",
    "    input_file = os.path.join(base_path, f\"{file_name}.txt\") # Changed from .TXT to .txt to match uploaded filename 'Camaligan_Exp.txt'\n",
    "    output_excel_file = os.path.join(base_path, f\"{file_name}_Corrected.xlsx\") \n",
    "    output_txt_file = os.path.join(base_path, f\"{file_name}_Corrected.txt\") # New TXT output path for corrected file\n",
    "    \n",
    "    process_data_file(input_file, output_excel_file, output_txt_file, \n",
    "                      start_point=start_dt, \n",
    "                      end_point=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DAILY SUMMARY`\n",
    "\n",
    "This code snippet does the following:\n",
    "\n",
    "* Gives us an excel file and text file of the generalized 'daily' summary of the CORRECTED data for ease of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAILY SUMMARY#\n",
    "import datetime\n",
    "import collections\n",
    "import os\n",
    "import pandas as pd # Using pandas for easier DataFrame manipulation\n",
    "import io\n",
    "import base64 # For encoding Excel content to be displayable in this environment\n",
    "import openpyxl # For direct Excel file creation\n",
    "\n",
    "def _detect_delimiter_and_format(lines_to_sample, potential_delimiters=None, potential_date_formats=None):\n",
    "    \"\"\"\n",
    "    Analyzes sample lines to detect the most likely delimiter and date format.\n",
    "    \"\"\"\n",
    "    if potential_delimiters is None:\n",
    "        potential_delimiters = ['\\t', ',']\n",
    "\n",
    "    if potential_date_formats is None:\n",
    "        # Common formats: DD/MM/YYYY, MM/DD/YYYY, with and without leading zeros\n",
    "        potential_date_formats = [\"%d/%m/%Y\", \"%Y/%m/%d\", \"%#d/%#m/%Y\", \"%Y/%#m/%#d\"] # Added %#d/%#m/%Y for dates like 1/1/2024\n",
    "\n",
    "    best_delimiter = None\n",
    "    best_date_format_str = None\n",
    "    max_successful_parses = -1\n",
    "\n",
    "    for delim in potential_delimiters:\n",
    "        for date_fmt in potential_date_formats:\n",
    "            successful_parses = 0\n",
    "            for line in lines_to_sample:\n",
    "                line_parts = line.strip().split(delim)\n",
    "                # We need at least Date, Time, Reading, 10-min. Reading is at index 2 (0-indexed)\n",
    "                if len(line_parts) >= 4: # Checking for at least 4 parts (Date, Time, Reading, 10-min)\n",
    "                    date_str = line_parts[0].strip()\n",
    "                    time_str = line_parts[1].strip()\n",
    "                    value_str = line_parts[2].strip() # Reading is at index 2\n",
    "\n",
    "                    try:\n",
    "                        # Attempt to parse combined date and time\n",
    "                        datetime.datetime.strptime(f\"{date_str} {time_str}\", f\"{date_fmt} %H:%M\")\n",
    "                        # Attempt to convert reading to float, allowing 'NC' for now\n",
    "                        if value_str != 'NC':\n",
    "                            float(value_str)\n",
    "                        successful_parses += 1\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                    except IndexError: # Not enough parts after splitting\n",
    "                        pass\n",
    "            \n",
    "            if successful_parses > max_successful_parses:\n",
    "                max_successful_parses = successful_parses\n",
    "                best_delimiter = delim\n",
    "                best_date_format_str = f\"{date_fmt} %H:%M\"\n",
    "\n",
    "    if max_successful_parses == 0:\n",
    "        raise ValueError(\"Could not detect suitable delimiter and date format from sample lines. Please ensure the file has data in expected formats.\")\n",
    "\n",
    "    return best_delimiter, best_date_format_str\n",
    "\n",
    "def get_daily_accumulation_end_dt(record_dt):\n",
    "    \"\"\"\n",
    "    Determines the 00:00 datetime that marks the end of the daily accumulation period\n",
    "    to which the given record_dt contributes.\n",
    "    For example, 2024/01/01 10:00 -> 2024/01/02 00:00\n",
    "    2024/01/01 00:00 -> 2024/01/01 00:00 (this 00:00 is the end of the previous day's accumulation)\n",
    "    \"\"\"\n",
    "    if record_dt.hour == 0 and record_dt.minute == 0:\n",
    "        # If it's 00:00, it marks the end of the *previous* day's accumulation.\n",
    "        # So for a 00:00 timestamp, the relevant daily accumulation belongs to the date of that 00:00.\n",
    "        return record_dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    \n",
    "    # For any other time, it contributes to the accumulation that ends at 00:00 of the next day.\n",
    "    next_day = record_dt.date() + datetime.timedelta(days=1)\n",
    "    return datetime.datetime(next_day.year, next_day.month, next_day.day, 0, 0)\n",
    "\n",
    "\n",
    "def process_single_file_for_daily_summary(file_content, filename):\n",
    "    \"\"\"\n",
    "    Reads a single file's content, processes it, and returns the daily summary\n",
    "    in the requested format (Date, Daily Accumulation, Summary (Rain/No Rain)).\n",
    "\n",
    "    Args:\n",
    "        file_content (str): The raw string content of the input file.\n",
    "        filename (str): The name of the file (used for logging and output naming).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "               - str: The content for the text summary file.\n",
    "               - str: The base64 encoded content for the Excel summary file.\n",
    "               Returns (None, None) if processing fails.\n",
    "    \"\"\"\n",
    "\n",
    "    actual_data_records = collections.OrderedDict()\n",
    "    \n",
    "    initial_header_line = None\n",
    "    start_data_line_index = 0\n",
    "    file_delimiter = None\n",
    "    input_datetime_format = None\n",
    "\n",
    "    lines = file_content.strip().split('\\n')\n",
    "    \n",
    "    if not lines:\n",
    "        print(f\"Error: Empty content for file '{filename}'.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Attempt to detect initial ID line (e.g., \"630381\")\n",
    "    first_line_stripped = lines[0].strip()\n",
    "    temp_parts_tab = first_line_stripped.split('\\t')\n",
    "    temp_parts_comma = first_line_stripped.split(',')\n",
    "\n",
    "    is_header_tab = len(temp_parts_tab) >= 1 and temp_parts_tab[0].isdigit() and all(p == '' for p in temp_parts_tab[1:])\n",
    "    is_header_comma = len(temp_parts_comma) >= 1 and temp_parts_comma[0].isdigit() and all(p == '' for p in temp_parts_comma[1:])\n",
    "    \n",
    "    if is_header_tab or is_header_comma:\n",
    "        initial_header_line = lines[0]\n",
    "        start_data_line_index = 1\n",
    "        \n",
    "    sample_lines = lines[start_data_line_index : min(start_data_line_index + 20, len(lines))]\n",
    "    \n",
    "    if not sample_lines:\n",
    "        print(f\"Error: No data lines found in the input content for '{filename}' for format detection.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        file_delimiter, input_datetime_format = _detect_delimiter_and_format(sample_lines)\n",
    "        print(f\"Auto-detected Delimiter: {repr(file_delimiter)}, Date/Time Format: '{input_datetime_format}' for {filename}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during auto-detection for {filename}: {e}\")\n",
    "        print(\"Attempting to proceed with default settings (tab delimiter, DD/MM/YYYY HH:MM).\")\n",
    "        file_delimiter = '\\t'\n",
    "        input_datetime_format = \"%d/%m/%Y %H:%M\"\n",
    "\n",
    "    for line_num, line in enumerate(lines[start_data_line_index:]):\n",
    "        line_parts = line.strip().split(file_delimiter)\n",
    "        \n",
    "        # Expecting at least Date, Time, Reading, 10-min difference (index 0, 1, 2, 3)\n",
    "        if len(line_parts) >= 4:\n",
    "            try:\n",
    "                current_datetime = datetime.datetime.strptime(f\"{line_parts[0].strip()} {line_parts[1].strip()}\", input_datetime_format)\n",
    "                \n",
    "                try:\n",
    "                    # Reading value is at index 2 (Reading column)\n",
    "                    reading_value_str = line_parts[2].strip()\n",
    "                    reading_value = float(reading_value_str) if reading_value_str != 'NC' else None # Store None for 'NC'\n",
    "                except ValueError:\n",
    "                    reading_value = None # Explicitly set to None if parsing fails\n",
    "                \n",
    "                actual_data_records[current_datetime] = reading_value\n",
    "\n",
    "            except ValueError as e:\n",
    "                print(f\"Warning: Skipping malformed line {line_num + start_data_line_index + 1} in {filename}: '{line.strip()}' - Error parsing date/time or value: {e}\")\n",
    "            except IndexError:\n",
    "                print(f\"Warning: Skipping incomplete line {line_num + start_data_line_index + 1} in {filename}: '{line.strip()}' - Not enough columns.\")\n",
    "        else:\n",
    "            print(f\"Warning: Skipping malformed line {line_num + start_data_line_index + 1} in {filename}: '{line.strip()}' - Does not have expected number of columns (Date, Time, Reading, 10-min).\")\n",
    "\n",
    "    all_parsed_datetimes = sorted(actual_data_records.keys())\n",
    "\n",
    "    if not all_parsed_datetimes:\n",
    "        print(f\"No valid data found or processed from the input file '{filename}' to reconstruct.\")\n",
    "        return None, None\n",
    "\n",
    "    # Determine start and end points for filling intervals\n",
    "    first_data_dt_from_file = all_parsed_datetimes[0]\n",
    "    last_data_dt_from_file = all_parsed_datetimes[-1]\n",
    "\n",
    "    # Adjust start and end points to nearest 10-minute interval\n",
    "    output_start_dt = first_data_dt_from_file.replace(minute=(first_data_dt_from_file.minute // 10) * 10, second=0, microsecond=0)\n",
    "    if output_start_dt.minute % 10 != 0: # If it wasn't already on a 10-min mark, round up\n",
    "        output_start_dt += datetime.timedelta(minutes=(10 - (output_start_dt.minute % 10)))\n",
    "\n",
    "    output_end_dt = last_data_dt_from_file.replace(minute=(last_data_dt_from_file.minute // 10) * 10, second=0, microsecond=0)\n",
    "    # No need to round up end_dt for completeness if it's already on a 10-min mark or rounded down.\n",
    "\n",
    "    # Fill missing intervals\n",
    "    current_expected_dt = output_start_dt\n",
    "    records_with_readings = []\n",
    "    \n",
    "    while current_expected_dt <= output_end_dt:\n",
    "        reading = actual_data_records.get(current_expected_dt)\n",
    "        records_with_readings.append({\n",
    "            'datetime': current_expected_dt,\n",
    "            'reading': reading if reading is not None else \"NC\" # Use \"NC\" if no reading or parsed as None\n",
    "        })\n",
    "        current_expected_dt += datetime.timedelta(minutes=10)\n",
    "\n",
    "    # Sort again just in case, though the loop should keep it sorted\n",
    "    records_with_readings.sort(key=lambda x: x['datetime'])\n",
    "\n",
    "    # Calculate 10-min differences and hourly accumulation\n",
    "    # For daily summary, we mainly care about 'Daily Accumulation', but these intermediate\n",
    "    # steps are part of the original logic you provided to derive it.\n",
    "    \n",
    "    # Stage 3 & 4: Calculate 10-min differences and Hourly Accumulation/Determinant\n",
    "    # This part needs to ensure previous_numeric_reading correctly handles \"NC\"\n",
    "    \n",
    "    # Initialize previous_numeric_reading\n",
    "    previous_numeric_reading = None\n",
    "    for rec in records_with_readings:\n",
    "        if rec['reading'] != \"NC\":\n",
    "            previous_numeric_reading = rec['reading']\n",
    "            break # Found the first valid reading\n",
    "\n",
    "    if previous_numeric_reading is None and records_with_readings:\n",
    "        # All readings are 'NC' or None, handle this case\n",
    "        print(f\"Warning: No numeric readings found in {filename} to calculate differences.\")\n",
    "\n",
    "\n",
    "    # Now iterate to calculate 10-min diffs and hourly accumulation\n",
    "    hourly_diffs_window = collections.deque(maxlen=6) # Use deque for efficient windowing\n",
    "\n",
    "    for i, record in enumerate(records_with_readings):\n",
    "        current_reading_val = record['reading']\n",
    "        ten_min_diff = \"NC\"\n",
    "\n",
    "        if current_reading_val != \"NC\":\n",
    "            if previous_numeric_reading is not None and i > 0: # Only calculate diff if there's a previous numeric reading\n",
    "                ten_min_diff = current_reading_val - previous_numeric_reading\n",
    "            else: # First valid reading, or first reading overall if no previous numeric\n",
    "                ten_min_diff = 0\n",
    "            previous_numeric_reading = current_reading_val # Update previous numeric reading\n",
    "        \n",
    "        # Add ten_min_diff to the record (even if \"NC\") for subsequent calculations\n",
    "        record['ten_min_diff'] = ten_min_diff\n",
    "        \n",
    "        # Hourly accumulation logic\n",
    "        hourly_diffs_window.append(ten_min_diff)\n",
    "\n",
    "        # Check if it's the end of an hour (every 6th 10-min interval, starting from 00:00)\n",
    "        # Or specifically, when the minute is 00 (e.g., 01:00, 02:00, etc.)\n",
    "        if record['datetime'].minute == 0 and record['datetime'] != output_start_dt: # Not the very first record unless it's 00:00\n",
    "            current_hourly_accumulation = 0\n",
    "            has_nc_in_window = False\n",
    "            \n",
    "            # Sum the last 6 (including current 0-min diff) to get hourly total\n",
    "            # We need to look back 6 entries including the current one.\n",
    "            temp_window_for_hourly_calc = []\n",
    "            for j in range(6):\n",
    "                if i - j >= 0:\n",
    "                    temp_window_for_hourly_calc.insert(0, records_with_readings[i-j]['ten_min_diff'])\n",
    "                else: # Pad with 0 for records before start_dt in the first hour\n",
    "                    temp_window_for_hourly_calc.insert(0, 0) \n",
    "            \n",
    "            for diff in temp_window_for_hourly_calc:\n",
    "                if diff == \"NC\":\n",
    "                    has_nc_in_window = True\n",
    "                    break\n",
    "                else:\n",
    "                    current_hourly_accumulation += diff\n",
    "            \n",
    "            if has_nc_in_window:\n",
    "                record['hourly_accumulation'] = \"NC\"\n",
    "                record['accumulation_determinant'] = \"NC\"\n",
    "            else:\n",
    "                record['hourly_accumulation'] = current_hourly_accumulation\n",
    "                if current_hourly_accumulation == 0:\n",
    "                    record['accumulation_determinant'] = \"No Accumulation\"\n",
    "                elif current_hourly_accumulation > 0:\n",
    "                    record['accumulation_determinant'] = \"Accumulation\"\n",
    "                else:\n",
    "                    record['accumulation_determinant'] = \"Decline\"\n",
    "        else:\n",
    "             # These are intermediate records, no hourly summary for them\n",
    "            record['hourly_accumulation'] = \"\"\n",
    "            record['accumulation_determinant'] = \"\"\n",
    "\n",
    "    # Stage 5: Calculate Daily Accumulation (00:10 to 00:00 next day)\n",
    "    daily_sums_to_assign = collections.defaultdict(lambda: {'sum': 0.0, 'has_nc': False, 'last_record_dt': None})\n",
    "\n",
    "    for record in records_with_readings:\n",
    "        hourly_acc = record.get('hourly_accumulation') # Use .get to safely access\n",
    "        record_dt = record['datetime']\n",
    "\n",
    "        # Determine which \"day\" this record contributes to, ending at a 00:00 timestamp\n",
    "        daily_period_end_dt = get_daily_accumulation_end_dt(record_dt)\n",
    "\n",
    "        if hourly_acc != \"\" and hourly_acc is not None: # Only process actual hourly accumulation values\n",
    "            if hourly_acc == \"NC\":\n",
    "                daily_sums_to_assign[daily_period_end_dt]['has_nc'] = True\n",
    "            elif isinstance(hourly_acc, (int, float)):\n",
    "                if not daily_sums_to_assign[daily_period_end_dt]['has_nc']:\n",
    "                    daily_sums_to_assign[daily_period_end_dt]['sum'] += hourly_acc\n",
    "            # Store the latest timestamp for each daily period, to assign the daily total to it\n",
    "            daily_sums_to_assign[daily_period_end_dt]['last_record_dt'] = record_dt\n",
    "    \n",
    "    # Prepare the final list of daily summary records\n",
    "    final_daily_summaries = []\n",
    "\n",
    "    # Iterate through the calculated daily sums\n",
    "    for daily_end_dt, info in sorted(daily_sums_to_assign.items()):\n",
    "        daily_accumulation_value = info['sum']\n",
    "        has_nc = info['has_nc']\n",
    "        \n",
    "        # The prompt implies the daily summary is assigned to the date *of the period*.\n",
    "        # So, if accumulation ends at 2024/01/01 00:00, that's the daily summary for 2023/12/31.\n",
    "        # If the accumulation ends at 2024/01/02 00:00, that's the daily summary for 2024/01/01.\n",
    "        summary_date = daily_end_dt.date() if (daily_end_dt.hour == 0 and daily_end_dt.minute == 0) else daily_end_dt.date() - datetime.timedelta(days=1)\n",
    "        \n",
    "        if has_nc:\n",
    "            daily_acc_output = \"NC\"\n",
    "            summary_rain_no_rain = \"Incomplete (NC in hourly data)\"\n",
    "        else:\n",
    "            daily_acc_output = round(daily_accumulation_value, 1) # Round to 1 decimal place\n",
    "            summary_rain_no_rain = \"Rain\" if daily_accumulation_value > 0 else \"No Rain\"\n",
    "            \n",
    "        # Refined Summary String including value and unit\n",
    "        if daily_acc_output != \"NC\":\n",
    "            summary_rain_no_rain_formatted = f\"{summary_rain_no_rain} ({daily_acc_output:.1f} mm)\"\n",
    "        else:\n",
    "            summary_rain_no_rain_formatted = summary_rain_no_rain # Keep as \"Incomplete (NC...)\"\n",
    "\n",
    "\n",
    "        final_daily_summaries.append({\n",
    "            'Date': summary_date.strftime(\"%d/%m/%Y\"),\n",
    "            'Daily Accumulation': daily_acc_output,\n",
    "            'Summary (Rain/No Rain)': summary_rain_no_rain_formatted\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame for the final summary (optional, but convenient for text/Excel consistency)\n",
    "    if not final_daily_summaries:\n",
    "        print(f\"No daily summaries could be generated for {filename}.\")\n",
    "        return None, None\n",
    "        \n",
    "    summary_df = pd.DataFrame(final_daily_summaries)\n",
    "\n",
    "    # --- Generate Text File Content for the summary ---\n",
    "    text_content = f\"Daily Rainfall Summary for {filename}\\n\"\n",
    "    text_content += \"------------------------------------------------------------------------\\n\"\n",
    "    text_content += \"Date            Daily Accumulation                Summary (Rain/No Rain)\\n\"\n",
    "    text_content += \"-\" * 70 + \"\\n\"\n",
    "    for _, row in summary_df.iterrows():\n",
    "        # Ensure consistent formatting for Daily Accumulation (e.g., \"1.5\" instead of \"1.500000\")\n",
    "        daily_acc_str = f\"{row['Daily Accumulation']:.1f}\" if isinstance(row['Daily Accumulation'], (int, float)) else str(row['Daily Accumulation'])\n",
    "        text_content += f\"{row['Date']:<15} {daily_acc_str:<25} {row['Summary (Rain/No Rain)']}\\n\"\n",
    "\n",
    "    # --- Generate Excel File Content (Base64 Encoded) for the summary using openpyxl ---\n",
    "    output_excel = io.BytesIO()\n",
    "    workbook = openpyxl.Workbook()\n",
    "    sheet = workbook.active\n",
    "    sheet.title = \"Daily Rainfall Summary\"\n",
    "\n",
    "    headers = [\"Date\", \"Daily Accumulation\", \"Summary (Rain/No Rain)\"]\n",
    "    sheet.append(headers)\n",
    "\n",
    "    for _, row in summary_df.iterrows():\n",
    "        # Ensure 'Daily Accumulation' is stored as a number in Excel if possible\n",
    "        daily_acc_for_excel = row['Daily Accumulation']\n",
    "        if isinstance(daily_acc_for_excel, str) and daily_acc_for_excel == \"NC\":\n",
    "            daily_acc_for_excel = \"NC\" # Keep as string if it's \"NC\"\n",
    "        \n",
    "        sheet.append([\n",
    "            row['Date'],\n",
    "            daily_acc_for_excel,\n",
    "            row['Summary (Rain/No Rain)']\n",
    "        ])\n",
    "            \n",
    "    workbook.save(output_excel)\n",
    "    excel_content_base64 = base64.b64encode(output_excel.getvalue()).decode('utf-8')\n",
    "\n",
    "    return text_content, excel_content_base64\n",
    "\n",
    "# --- HOW TO USE THIS CODE ON YOUR LOCAL MACHINE ---\n",
    "\n",
    "# 1. Save this code:\n",
    "#    Save the entire content of this immersive block as a Python file (e.g., `process_rainfall_summaries.py`)\n",
    "#    on your computer.\n",
    "\n",
    "# 2. Prepare your files:\n",
    "#    Place all your rainfall data text files (e.g., Alanao_Exp.TXT, Bagamelon_Exp.TXT)\n",
    "#    into the specified `base_path` folder on your computer.\n",
    "\n",
    "# 3. Specify your base folder path:\n",
    "#    Change the `base_path` variable below to the actual path of the folder\n",
    "#    where your rainfall data text files are located.\n",
    "#    Example: base_path = 'C:/Users/YourUser/Documents/RainfallData/' (Windows)\n",
    "#    Example: base_path = '/Users/YourUser/Documents/RainfallData/' (macOS/Linux)\n",
    "#    Note: Ensure the path ends with a backslash or forward slash.\n",
    "base_path = r\"D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\\" # <<<<<<< IMPORTANT: CHANGE THIS TO YOUR ACTUAL FOLDER PATH\n",
    "\n",
    "# 4. List the files to process:\n",
    "#    This list maps file base names to their explicit processing start times.\n",
    "#    The script will automatically append \"_Exp.TXT\" to these base names to find the input files.\n",
    "files_to_process = [\n",
    "    (\"Alanao\", datetime.datetime(2019, 4, 25, 11, 10)),\n",
    "    (\"Bagamelon\", datetime.datetime(2019, 4, 25, 14, 10)),\n",
    "    (\"Balongay\", datetime.datetime(2019, 4, 26, 11, 40)),\n",
    "    (\"Bato\", datetime.datetime(2020, 12, 17, 12, 10)),\n",
    "    (\"Buhi\", datetime.datetime(2020, 12, 17, 10, 10)),\n",
    "    (\"Caguscos\", datetime.datetime(2020, 12, 18, 9, 10)),\n",
    "    (\"Calzada\", datetime.datetime(2020, 12, 18, 13, 10)),\n",
    "    (\"Camaligan\", datetime.datetime(2019, 4, 26, 10, 30)),\n",
    "    (\"Malabog\", datetime.datetime(2020, 12, 18, 12, 10)),\n",
    "    (\"Ocampo\", datetime.datetime(2020, 12, 27, 10, 10)),\n",
    "    (\"Ombao\", datetime.datetime(2024, 12, 5, 15, 10)),\n",
    "    (\"Sipocot\", datetime.datetime(2019, 4, 25, 13, 10))\n",
    "]\n",
    "\n",
    "# 5. Define an output directory for the summaries:\n",
    "output_summary_directory = os.path.join(base_path, \"Daily_Summaries\") # Creates a subfolder for summaries\n",
    "os.makedirs(output_summary_directory, exist_ok=True) # Ensure the output directory exists\n",
    "\n",
    "# 6. Process each file and save its summary:\n",
    "print(f\"Output summaries will be saved in: {output_summary_directory}\\n\")\n",
    "\n",
    "for file_base_name, _ in files_to_process:\n",
    "    # Construct the actual input filename with \"_Exp.TXT\"\n",
    "    actual_input_filename = f\"{file_base_name}_Corrected.TXT\"\n",
    "    print(f\"\\n--- Processing {actual_input_filename} ---\")\n",
    "    input_file_path = os.path.join(base_path, actual_input_filename)\n",
    "    \n",
    "    if not os.path.exists(input_file_path):\n",
    "        print(f\"Error: Input file '{input_file_path}' not found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(input_file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            file_content = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading content of '{input_file_path}': {e}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Process the content to get the summary outputs, passing the full filename\n",
    "    text_summary, excel_summary_base64 = process_single_file_for_daily_summary(file_content, actual_input_filename)\n",
    "\n",
    "    if text_summary is None or excel_summary_base64 is None:\n",
    "        print(f\"Could not generate summary for {actual_input_filename}. See warnings above.\")\n",
    "        continue\n",
    "\n",
    "    # Save the text summary file\n",
    "    # Output file names will be \"Alanao_Daily_Summary.txt\"\n",
    "    output_txt_file = os.path.join(output_summary_directory, f\"{file_base_name}_Daily_Summary.txt\")\n",
    "    try:\n",
    "        with open(output_txt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text_summary)\n",
    "        print(f\"Text summary for {actual_input_filename} saved to: {output_txt_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving text summary for {actual_input_filename}: {e}\")\n",
    "\n",
    "    # Save the Excel summary file\n",
    "    # Output file names will be \"Alanao_Daily_Summary.xlsx\"\n",
    "    output_excel_file = os.path.join(output_summary_directory, f\"{file_base_name}_Daily_Summary.xlsx\")\n",
    "    try:\n",
    "        decoded_excel_bytes = base64.b64decode(excel_summary_base64)\n",
    "        with open(output_excel_file, \"wb\") as f:\n",
    "            f.write(decoded_excel_bytes)\n",
    "        print(f\"Excel summary for {actual_input_filename} saved to: {output_excel_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Excel summary for {actual_input_filename}: {e}\")\n",
    "\n",
    "print(\"\\n--- All file processing complete. ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HOURLY SUMMARY`\n",
    "\n",
    "This code snippet does the following:\n",
    "\n",
    "* Gives us an excel file and text file of the generalized 'hourly' summary of the CORRECTED data for ease of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOURLY SUMMARY #\n",
    "\n",
    "import datetime\n",
    "import collections\n",
    "import os\n",
    "import pandas as pd # Using pandas for easier DataFrame manipulation\n",
    "import io\n",
    "import base64 # For encoding Excel content to be displayable in this environment\n",
    "import openpyxl # For direct Excel file creation\n",
    "\n",
    "def _detect_delimiter_and_format(lines_to_sample, potential_delimiters=None, potential_date_formats=None):\n",
    "    \"\"\"\n",
    "    Analyzes sample lines to detect the most likely delimiter and date format.\n",
    "    \"\"\"\n",
    "    if potential_delimiters is None:\n",
    "        potential_delimiters = ['\\t', ',']\n",
    "\n",
    "    if potential_date_formats is None:\n",
    "        # Common formats: DD/MM/YYYY, YYYY/MM/DD, with and without leading zeros\n",
    "        potential_date_formats = [\"%d/%m/%Y\", \"%Y/%m/%d\", \"%#d/%#m/%Y\", \"%Y/%#m/%#d\"] # Added %#d/%#m/%Y for dates like 1/1/2024\n",
    "\n",
    "    best_delimiter = None\n",
    "    best_date_format_str = None\n",
    "    max_successful_parses = -1\n",
    "\n",
    "    for delim in potential_delimiters:\n",
    "        for date_fmt in potential_date_formats:\n",
    "            successful_parses = 0\n",
    "            for line in lines_to_sample:\n",
    "                line_parts = line.strip().split(delim)\n",
    "                # We need at least Date, Time, Reading, 10-min. Reading is at index 2 (0-indexed)\n",
    "                if len(line_parts) >= 4: # Checking for at least 4 parts (Date, Time, Reading, 10-min)\n",
    "                    date_str = line_parts[0].strip()\n",
    "                    time_str = line_parts[1].strip()\n",
    "                    value_str = line_parts[2].strip() # Reading is at index 2\n",
    "\n",
    "                    try:\n",
    "                        # Attempt to parse combined date and time\n",
    "                        datetime.datetime.strptime(f\"{date_str} {time_str}\", f\"{date_fmt} %H:%M\")\n",
    "                        # Attempt to convert reading to float, allowing 'NC' for now\n",
    "                        if value_str != 'NC':\n",
    "                            float(value_str)\n",
    "                        successful_parses += 1\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                    except IndexError: # Not enough parts after splitting\n",
    "                        pass\n",
    "            \n",
    "            if successful_parses > max_successful_parses:\n",
    "                max_successful_parses = successful_parses\n",
    "                best_delimiter = delim\n",
    "                best_date_format_str = f\"{date_fmt} %H:%M\"\n",
    "\n",
    "    if max_successful_parses == 0:\n",
    "        raise ValueError(\"Could not detect suitable delimiter and date format from sample lines. Please ensure the file has data in expected formats.\")\n",
    "\n",
    "    return best_delimiter, best_date_format_str\n",
    "\n",
    "def get_daily_accumulation_end_dt(record_dt):\n",
    "    \"\"\"\n",
    "    Determines the 00:00 datetime that marks the end of the daily accumulation period\n",
    "    to which the given record_dt contributes.\n",
    "    For example, 2024/01/01 10:00 -> 2024/01/02 00:00\n",
    "    2024/01/01 00:00 -> 2024/01/01 00:00 (this 00:00 is the end of the previous day's accumulation)\n",
    "    \"\"\"\n",
    "    # This function is retained from the original code but is no longer directly used for hourly summaries.\n",
    "    # It would be used if calculating daily totals from hourly ones, but for just hourly, it's not strictly necessary.\n",
    "    if record_dt.hour == 0 and record_dt.minute == 0:\n",
    "        return record_dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    \n",
    "    next_day = record_dt.date() + datetime.timedelta(days=1)\n",
    "    return datetime.datetime(next_day.year, next_day.month, next_day.day, 0, 0)\n",
    "\n",
    "\n",
    "def process_single_file_for_hourly_summary(file_content, filename):\n",
    "    \"\"\"\n",
    "    Reads a single file's content, processes it, and returns the hourly summary\n",
    "    in the requested format (Date, Time, Hourly Accumulation, Summary (Rain/No Rain)).\n",
    "\n",
    "    Args:\n",
    "        file_content (str): The raw string content of the input file.\n",
    "        filename (str): The name of the file (used for logging and output naming).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "               - str: The content for the text summary file.\n",
    "               - str: The base64 encoded content for the Excel summary file.\n",
    "               Returns (None, None) if processing fails.\n",
    "    \"\"\"\n",
    "\n",
    "    actual_data_records = collections.OrderedDict()\n",
    "    \n",
    "    initial_header_line = None\n",
    "    start_data_line_index = 0\n",
    "    file_delimiter = None\n",
    "    input_datetime_format = None\n",
    "\n",
    "    lines = file_content.strip().split('\\n')\n",
    "    \n",
    "    if not lines:\n",
    "        print(f\"Error: Empty content for file '{filename}'.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Attempt to detect initial ID line (e.g., \"630381\")\n",
    "    first_line_stripped = lines[0].strip()\n",
    "    temp_parts_tab = first_line_stripped.split('\\t')\n",
    "    temp_parts_comma = first_line_stripped.split(',')\n",
    "\n",
    "    is_header_tab = len(temp_parts_tab) >= 1 and temp_parts_tab[0].isdigit() and all(p == '' for p in temp_parts_tab[1:])\n",
    "    is_header_comma = len(temp_parts_comma) >= 1 and temp_parts_comma[0].isdigit() and all(p == '' for p in temp_parts_comma[1:])\n",
    "    \n",
    "    if is_header_tab or is_header_comma:\n",
    "        initial_header_line = lines[0]\n",
    "        start_data_line_index = 1\n",
    "        \n",
    "    sample_lines = lines[start_data_line_index : min(start_data_line_index + 20, len(lines))]\n",
    "    \n",
    "    if not sample_lines:\n",
    "        print(f\"Error: No data lines found in the input content for '{filename}' for format detection.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        file_delimiter, input_datetime_format = _detect_delimiter_and_format(sample_lines)\n",
    "        print(f\"Auto-detected Delimiter: {repr(file_delimiter)}, Date/Time Format: '{input_datetime_format}' for {filename}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during auto-detection for {filename}: {e}\")\n",
    "        print(\"Attempting to proceed with default settings (tab delimiter, DD/MM/YYYY HH:MM).\")\n",
    "        file_delimiter = '\\t'\n",
    "        input_datetime_format = \"%d/%m/%Y %H:%M\"\n",
    "\n",
    "    for line_num, line in enumerate(lines[start_data_line_index:]):\n",
    "        line_parts = line.strip().split(file_delimiter)\n",
    "        \n",
    "        # Expecting at least Date, Time, Reading, 10-min difference (index 0, 1, 2, 3)\n",
    "        if len(line_parts) >= 4:\n",
    "            try:\n",
    "                current_datetime = datetime.datetime.strptime(f\"{line_parts[0].strip()} {line_parts[1].strip()}\", input_datetime_format)\n",
    "                \n",
    "                try:\n",
    "                    # Reading value is at index 2 (Reading column)\n",
    "                    reading_value_str = line_parts[2].strip()\n",
    "                    reading_value = float(reading_value_str) if reading_value_str != 'NC' else None # Store None for 'NC'\n",
    "                except ValueError:\n",
    "                    reading_value = None # Explicitly set to None if parsing fails\n",
    "                \n",
    "                actual_data_records[current_datetime] = reading_value\n",
    "\n",
    "            except ValueError as e:\n",
    "                print(f\"Warning: Skipping malformed line {line_num + start_data_line_index + 1} in {filename}: '{line.strip()}' - Error parsing date/time or value: {e}\")\n",
    "            except IndexError:\n",
    "                print(f\"Warning: Skipping incomplete line {line_num + start_data_line_index + 1} in {filename}: '{line.strip()}' - Not enough columns.\")\n",
    "        else:\n",
    "            print(f\"Warning: Skipping malformed line {line_num + start_data_line_index + 1} in {filename}: '{line.strip()}' - Does not have expected number of columns (Date, Time, Reading, 10-min).\")\n",
    "\n",
    "    all_parsed_datetimes = sorted(actual_data_records.keys())\n",
    "\n",
    "    if not all_parsed_datetimes:\n",
    "        print(f\"No valid data found or processed from the input file '{filename}' to reconstruct.\")\n",
    "        return None, None\n",
    "\n",
    "    # Determine start and end points for filling intervals\n",
    "    first_data_dt_from_file = all_parsed_datetimes[0]\n",
    "    last_data_dt_from_file = all_parsed_datetimes[-1]\n",
    "\n",
    "    # Adjust start and end points to nearest 10-minute interval\n",
    "    output_start_dt = first_data_dt_from_file.replace(minute=(first_data_dt_from_file.minute // 10) * 10, second=0, microsecond=0)\n",
    "    if output_start_dt.minute % 10 != 0: # If it wasn't already on a 10-min mark, round up\n",
    "        output_start_dt += datetime.timedelta(minutes=(10 - (output_start_dt.minute % 10)))\n",
    "\n",
    "    output_end_dt = last_data_dt_from_file.replace(minute=(last_data_dt_from_file.minute // 10) * 10, second=0, microsecond=0)\n",
    "\n",
    "    # Fill missing intervals\n",
    "    current_expected_dt = output_start_dt\n",
    "    records_with_readings = []\n",
    "    \n",
    "    while current_expected_dt <= output_end_dt:\n",
    "        reading = actual_data_records.get(current_expected_dt)\n",
    "        records_with_readings.append({\n",
    "            'datetime': current_expected_dt,\n",
    "            'reading': reading if reading is not None else \"NC\" # Use \"NC\" if no reading or parsed as None\n",
    "        })\n",
    "        current_expected_dt += datetime.timedelta(minutes=10)\n",
    "\n",
    "    # Sort again just in case, though the loop should keep it sorted\n",
    "    records_with_readings.sort(key=lambda x: x['datetime'])\n",
    "\n",
    "    # Initialize previous_numeric_reading for 10-min diff calculations\n",
    "    previous_numeric_reading = None\n",
    "    for rec in records_with_readings:\n",
    "        if rec['reading'] != \"NC\":\n",
    "            previous_numeric_reading = rec['reading']\n",
    "            break # Found the first valid reading\n",
    "\n",
    "    if previous_numeric_reading is None and records_with_readings:\n",
    "        print(f\"Warning: No numeric readings found in {filename} to calculate differences.\")\n",
    "\n",
    "    # Calculate 10-min differences and Hourly Accumulation/Determinant\n",
    "    for i, record in enumerate(records_with_readings):\n",
    "        current_reading_val = record['reading']\n",
    "        ten_min_diff = \"NC\"\n",
    "\n",
    "        if current_reading_val != \"NC\":\n",
    "            if previous_numeric_reading is not None and i > 0:\n",
    "                ten_min_diff = current_reading_val - previous_numeric_reading\n",
    "            else:\n",
    "                ten_min_diff = 0\n",
    "            previous_numeric_reading = current_reading_val\n",
    "        \n",
    "        record['ten_min_diff'] = ten_min_diff\n",
    "        \n",
    "        # Hourly accumulation logic: calculate at the top of the hour (e.g., 01:00, 02:00)\n",
    "        # This record is the *end* of the hourly period (e.g., 01:00 covers 00:10 to 01:00)\n",
    "        if record['datetime'].minute == 0:\n",
    "            current_hourly_accumulation = 0\n",
    "            has_nc_in_window = False\n",
    "            \n",
    "            # Look back 6 entries including the current one for the hourly window\n",
    "            temp_window_for_hourly_calc = []\n",
    "            for j in range(6):\n",
    "                # Ensure we don't go out of bounds at the very beginning of the data\n",
    "                if i - j >= 0:\n",
    "                    temp_window_for_hourly_calc.insert(0, records_with_readings[i-j]['ten_min_diff'])\n",
    "                else:\n",
    "                    # If we don't have a full 6 records at the start, treat missing as 0\n",
    "                    temp_window_for_hourly_calc.insert(0, 0)\n",
    "            \n",
    "            for diff in temp_window_for_hourly_calc:\n",
    "                if diff == \"NC\":\n",
    "                    has_nc_in_window = True\n",
    "                    break\n",
    "                else:\n",
    "                    current_hourly_accumulation += diff\n",
    "            \n",
    "            if has_nc_in_window:\n",
    "                record['hourly_accumulation'] = \"NC\"\n",
    "                record['accumulation_determinant'] = \"NC\"\n",
    "            else:\n",
    "                record['hourly_accumulation'] = current_hourly_accumulation\n",
    "                if current_hourly_accumulation == 0:\n",
    "                    record['accumulation_determinant'] = \"No Accumulation\"\n",
    "                elif current_hourly_accumulation > 0:\n",
    "                    record['accumulation_determinant'] = \"Accumulation\"\n",
    "                else:\n",
    "                    record['accumulation_determinant'] = \"Decline\"\n",
    "        else:\n",
    "             # These are intermediate records, no hourly summary for them\n",
    "            record['hourly_accumulation'] = \"\"\n",
    "            record['accumulation_determinant'] = \"\"\n",
    "\n",
    "    # Stage 5: Prepare Hourly Summary Records\n",
    "    final_hourly_summaries = []\n",
    "\n",
    "    for record in records_with_readings:\n",
    "        # We only want records that represent the end of an hour's accumulation (e.g., 01:00, 02:00 etc.)\n",
    "        if record['datetime'].minute == 0:\n",
    "            hourly_acc_value = record.get('hourly_accumulation')\n",
    "            \n",
    "            if hourly_acc_value is not None and hourly_acc_value != \"\":\n",
    "                summary_date = record['datetime'].strftime(\"%d/%m/%Y\")\n",
    "                summary_time = record['datetime'].strftime(\"%H:%M\") # Format as HH:MM\n",
    "                \n",
    "                if hourly_acc_value == \"NC\":\n",
    "                    hourly_acc_output = \"NC\"\n",
    "                    summary_rain_no_rain = \"Incomplete (NC in 10-min data)\"\n",
    "                else:\n",
    "                    hourly_acc_output = round(hourly_acc_value, 1) # Round to 1 decimal place\n",
    "                    summary_rain_no_rain = \"Rain\" if hourly_acc_value > 0 else \"No Rain\"\n",
    "                \n",
    "                # Refined Summary String including value and unit\n",
    "                if hourly_acc_output != \"NC\":\n",
    "                    summary_rain_no_rain_formatted = f\"{summary_rain_no_rain} ({hourly_acc_output:.1f} mm)\"\n",
    "                else:\n",
    "                    summary_rain_no_rain_formatted = summary_rain_no_rain\n",
    "\n",
    "\n",
    "                final_hourly_summaries.append({\n",
    "                    'Date': summary_date,\n",
    "                    'Time': summary_time,\n",
    "                    'Hourly Accumulation': hourly_acc_output,\n",
    "                    'Summary (Rain/No Rain)': summary_rain_no_rain_formatted\n",
    "                })\n",
    "\n",
    "    if not final_hourly_summaries:\n",
    "        print(f\"No hourly summaries could be generated for {filename}.\")\n",
    "        return None, None\n",
    "        \n",
    "    # Convert to DataFrame for easier handling and sorting\n",
    "    summary_df = pd.DataFrame(final_hourly_summaries)\n",
    "    # Ensure sorting by Date and then Time by converting back to datetime objects temporarily\n",
    "    summary_df['TempDateTime'] = pd.to_datetime(summary_df['Date'] + ' ' + summary_df['Time'], format='%d/%m/%Y %H:%M')\n",
    "    summary_df = summary_df.sort_values(by='TempDateTime').drop(columns='TempDateTime')\n",
    "\n",
    "\n",
    "    # --- Generate Text File Content for the summary ---\n",
    "    text_content = f\"Hourly Rainfall Summary for {filename}\\n\"\n",
    "    text_content += \"------------------------------------------------------------------------\\n\"\n",
    "    text_content += \"Date            Time    Hourly Accumulation       Summary (Rain/No Rain)\\n\"\n",
    "    text_content += \"-\" * 70 + \"\\n\"\n",
    "    for _, row in summary_df.iterrows():\n",
    "        # Ensure consistent formatting for Hourly Accumulation\n",
    "        hourly_acc_str = f\"{row['Hourly Accumulation']:.1f}\" if isinstance(row['Hourly Accumulation'], (int, float)) else str(row['Hourly Accumulation'])\n",
    "        text_content += f\"{row['Date']:<15} {row['Time']:<7} {hourly_acc_str:<25} {row['Summary (Rain/No Rain)']}\\n\"\n",
    "\n",
    "    # --- Generate Excel File Content (Base64 Encoded) for the summary using openpyxl ---\n",
    "    output_excel = io.BytesIO()\n",
    "    workbook = openpyxl.Workbook()\n",
    "    sheet = workbook.active\n",
    "    sheet.title = \"Hourly Rainfall Summary\"\n",
    "\n",
    "    headers = [\"Date\", \"Time\", \"Hourly Accumulation\", \"Summary (Rain/No Rain)\"]\n",
    "    sheet.append(headers)\n",
    "\n",
    "    for _, row in summary_df.iterrows():\n",
    "        # Ensure 'Hourly Accumulation' is stored as a number in Excel if possible\n",
    "        hourly_acc_for_excel = row['Hourly Accumulation']\n",
    "        if isinstance(hourly_acc_for_excel, str) and hourly_acc_for_excel == \"NC\":\n",
    "            hourly_acc_for_excel = \"NC\" # Keep as string if it's \"NC\"\n",
    "        \n",
    "        sheet.append([\n",
    "            row['Date'],\n",
    "            row['Time'],\n",
    "            hourly_acc_for_excel,\n",
    "            row['Summary (Rain/No Rain)']\n",
    "        ])\n",
    "            \n",
    "    workbook.save(output_excel)\n",
    "    excel_content_base64 = base64.b64encode(output_excel.getvalue()).decode('utf-8')\n",
    "\n",
    "    return text_content, excel_content_base64\n",
    "\n",
    "# --- HOW TO USE THIS CODE ON YOUR LOCAL MACHINE ---\n",
    "\n",
    "# 1. Save this code:\n",
    "#    Save the entire content of this immersive block as a Python file (e.g., `process_rainfall_summaries.py`)\n",
    "#    on your computer.\n",
    "\n",
    "# 2. Prepare your files:\n",
    "#    Place all your rainfall data text files (e.g., Alanao_Exp.TXT, Bagamelon_Exp.TXT)\n",
    "#    into the specified `base_path` folder on your computer.\n",
    "\n",
    "# 3. Specify your base folder path:\n",
    "#    Change the `base_path` variable below to the actual path of the folder\n",
    "#    where your rainfall data text files are located.\n",
    "#    Example: base_path = 'C:/Users/YourUser/Documents/RainfallData/' (Windows)\n",
    "#    Example: base_path = '/Users/YourUser/Documents/RainfallData/' (macOS/Linux)\n",
    "#    Note: Ensure the path ends with a backslash or forward slash.\n",
    "base_path = r\"D:\\OJT\\BRB\\Fill_In_data\\2025\\Experiment\\\\\" # <<<<<<< IMPORTANT: CHANGE THIS TO YOUR ACTUAL FOLDER PATH\n",
    "\n",
    "# 4. List the files to process:\n",
    "#    This list maps file base names to their explicit processing start times.\n",
    "#    The script will automatically append \"_Exp.TXT\" to these base names to find the input files.\n",
    "files_to_process = [\n",
    "    (\"Alanao\", datetime.datetime(2019, 4, 25, 11, 10)),\n",
    "    (\"Bagamelon\", datetime.datetime(2019, 4, 25, 14, 10)),\n",
    "    (\"Balongay\", datetime.datetime(2019, 4, 26, 11, 40)),\n",
    "    (\"Bato\", datetime.datetime(2020, 12, 17, 12, 10)),\n",
    "    (\"Buhi\", datetime.datetime(2020, 12, 17, 10, 10)),\n",
    "    (\"Caguscos\", datetime.datetime(2020, 12, 18, 9, 10)),\n",
    "    (\"Calzada\", datetime.datetime(2020, 12, 18, 13, 10)),\n",
    "    (\"Camaligan\", datetime.datetime(2019, 4, 26, 10, 30)),\n",
    "    (\"Malabog\", datetime.datetime(2020, 12, 18, 12, 10)),\n",
    "    (\"Ocampo\", datetime.datetime(2020, 12, 27, 10, 10)),\n",
    "    (\"Ombao\", datetime.datetime(2024, 12, 5, 15, 10)),\n",
    "    (\"Sipocot\", datetime.datetime(2019, 4, 25, 13, 10))\n",
    "]\n",
    "\n",
    "# 5. Define an output directory for the summaries:\n",
    "output_summary_directory = os.path.join(base_path, \"Hourly_Summaries\") # Creates a subfolder for summaries\n",
    "os.makedirs(output_summary_directory, exist_ok=True) # Ensure the output directory exists\n",
    "\n",
    "# 6. Process each file and save its summary:\n",
    "print(f\"Output summaries will be saved in: {output_summary_directory}\\n\")\n",
    "\n",
    "for file_base_name, _ in files_to_process:\n",
    "    # Construct the actual input filename with \"_Exp.TXT\"\n",
    "    actual_input_filename = f\"{file_base_name}_Corrected.TXT\"\n",
    "    print(f\"\\n--- Processing {actual_input_filename} ---\")\n",
    "    input_file_path = os.path.join(base_path, actual_input_filename)\n",
    "    \n",
    "    if not os.path.exists(input_file_path):\n",
    "        print(f\"Error: Input file '{input_file_path}' not found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(input_file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            file_content = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading content of '{input_file_path}': {e}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Process the content to get the summary outputs, passing the full filename\n",
    "    text_summary, excel_summary_base64 = process_single_file_for_hourly_summary(file_content, actual_input_filename)\n",
    "\n",
    "    if text_summary is None or excel_summary_base64 is None:\n",
    "        print(f\"Could not generate summary for {actual_input_filename}. See warnings above.\")\n",
    "        continue\n",
    "\n",
    "    # Save the text summary file\n",
    "    # Output file names will be \"Alanao_Hourly_Summary.txt\"\n",
    "    output_txt_file = os.path.join(output_summary_directory, f\"{file_base_name}_Hourly_Summary.txt\")\n",
    "    try:\n",
    "        with open(output_txt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text_summary)\n",
    "        print(f\"Text summary for {actual_input_filename} saved to: {output_txt_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving text summary for {actual_input_filename}: {e}\")\n",
    "\n",
    "    # Save the Excel summary file\n",
    "    # Output file names will be \"Alanao_Hourly_Summary.xlsx\"\n",
    "    output_excel_file = os.path.join(output_summary_directory, f\"{file_base_name}_Hourly_Summary.xlsx\")\n",
    "    try:\n",
    "        decoded_excel_bytes = base64.b64decode(excel_summary_base64)\n",
    "        with open(output_excel_file, \"wb\") as f:\n",
    "            f.write(decoded_excel_bytes)\n",
    "        print(f\"Excel summary for {actual_input_filename} saved to: {output_excel_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Excel summary for {actual_input_filename}: {e}\")\n",
    "\n",
    "print(\"\\n--- All file processing complete. ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepared by: Carl Benjamin L. Buenaflor | Bicol University | Meteo_ID2022\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
